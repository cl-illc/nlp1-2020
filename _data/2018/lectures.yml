-
  layout: lecture
  selected: y
  date: 2020-10-28
  img: Live-1
  uid: intro
  title: "Live lecture: Introduction"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "Introduction to the course"
  background:
  discussion:
  slides: resources/slides/NLP1-lecture1.pdf
  video: 
  further: 
    - "Chapter 4: [Naive Bayes classification and sentiment](https://web.stanford.edu/~jurafsky/slp3/4.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: Week 1 (released on 2020-10-28)
  img: langmodels
  uid: lec2-1
  title: "Language models"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss language models, i.e. modelling word sequences."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture2.pdf
  video: https://webcolleges.uva.nl/Mediasite/Play/ed24f15464dc43dcbafdb9e8c0c700d81d
  further: 
    - "Chapter 3: [Language modelling with n-grams](https://web.stanford.edu/~jurafsky/slp3/3.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:
-
  layout: lecture
  selected: y
  date: Week 1 (released on 2020-10-28)
  img: PoS
  uid: lec2-2
  title: "Part-of-speech tagging"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss part-of-speech tagging, i.e. assigning to each word its grammatical category."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture2.pdf
  video: https://webcolleges.uva.nl/Mediasite/Play/c44f96cb9b18474ab23b66a40691fa641d
  further: 
    - "Chapter 8: [Part-of-speech tagging](https://web.stanford.edu/~jurafsky/slp3/8.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:  
-  
  layout: lecture
  selected: y
  date: 2020-11-04
  img: LiveQA
  uid: qa2
  title: "Live Q & A: Language models and part-of-speech tagging"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this Q & A session, we will discuss language models and part-of-speech tagging."
  background:
  discussion:
  slides: 
  video: 
  further:   
  code: 
  data:     
-
  layout: lecture
  selected: y
  date: Week 2 (released on 2020-11-04)
  img: Morphology
  uid: lec3
  title: "Morphological processing"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss morphological processing"
  background:
  discussion:
  slides: resources/slides/NLP1-lecture3.pdf
  video: https://webcolleges.uva.nl/Mediasite/Play/4bd1dada1d6143369f90057ae8e6358f1d
  further: 
    - "Lecture notes on moprphology are available [here](https://cl-illc.github.io/nlp1/resources/slides/Morphology-notes.pdf)"
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: Week 2 (released on 2020-11-04)
  img: Parsing
  uid: lec4
  title: "Syntax and formal grammars"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, I will introduce syntax and formal grammars."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture4.pdf
  video: https://webcolleges.uva.nl/Mediasite/Play/60b48ad9886f4e0aa997165b58ed43ff1d
  further: 
     - "Chapter 12: [Constituency grammars](https://web.stanford.edu/~jurafsky/slp3/12.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:    
-  
  layout: lecture
  selected: y
  date: Week 2 (released on 2020-11-04)
  img: Parsing
  uid: lec4
  title: "Syntactic parsing"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss syntactic parsing."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture4.pdf
  video: https://webcolleges.uva.nl/Mediasite/Play/5aafeaacb2944e409b7f1134b086f9dd1d
  further: 
     - "Chapter 13: [Constituency parsing](https://web.stanford.edu/~jurafsky/slp3/13.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:    
-  
  layout: lecture
  selected: y
  date: 2020-11-11
  img: LiveQA
  uid: qa2
  title: "Live Q & A: Morphology and syntax"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this Q & A session, we will discuss morphological processing and syntactic parsing."
  background:
  discussion:
  slides: resources/slides/NLP1-LiveQA-2.pdf
  video: 
  further:   
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: Week 3 (released on 2020-11-11)
  img: WordNet
  uid: lec4-2
  title: "Lexical semantics"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss lexical semantics, i.e. modelling the meaning of words."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture4b.pdf
  video: https://webcolleges.uva.nl/Mediasite/Play/6070333e272f4a989b0cef26a36df74c1d
  further: 
     - "Chapter 19: [Word Senses and WordNet](https://web.stanford.edu/~jurafsky/slp3/19.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: Week 3 (released on 2020-11-11)
  img: vectors
  uid: lec5
  title: "Distributional semantics"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will introduce statistical models of word meaning"
  background:
  discussion:
  slides: resources/slides/NLP1-lecture5.pdf
  video: https://webcolleges.uva.nl/Mediasite/Play/be334e63f2d34ba98168a1f025bc37ef1d
  further: 
    - "Chapter 6: [Vector semantics and embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: Week 3 (released on 2020-11-12)
  img: skip-gram
  uid: lec6
  title: "Generalisation and word embeddings"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss generalisation from words to semantic classes and learning dense vector representations - word embeddings."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture6.pdf
  further: 
    - "Chapter 6: [Vector semantics and embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf) in Jurafsky and Martin (3rd edition)."
    - "A gentle introduction to neural networks can be found [here](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)"
    - "The following paper provides a nice explanation of skip-gram with negative sampling: Yoav Goldberg and Omer Levy. [word2vec Explained: Deriving Mikolov et al.â€™s Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722.pdf)"
  video: https://webcolleges.uva.nl/Mediasite/Play/7982b66a4dba43ee848461fe37f6a3441d
  code: 
  data:   
-
  layout: lecture
  selected: y
  date: 2020-11-18
  img: Live-1
  uid: lec7
  title: "Live lecture: Compositional semantics and sentence representations"
  instructor: "Mario Giulianelli"
  note: 
  abstract: >
    "In this lecture, we will discuss compositional semantics, i.e. modelling the meaning of phrases and sentences, and learning neural representations of sentences."
  background:
  discussion:
  slides: resources/slides/NLP1-2020-lecture7.pdf
  further: 
    - "Chapter 7: [Neural networks and neural language models](https://web.stanford.edu/~jurafsky/slp3/7.pdf) in Jurafsky and Martin (3rd edition)."
    - "Chapter 9: [Sequence processing with recurrent neural networks](https://web.stanford.edu/~jurafsky/slp3/9.pdf) in Jurafsky and Martin (3rd edition)."
    - "A good and general reference for Neural Networks in NLP: Yoav Goldberg. [A Primer on Neural Network Models for Natural Language Processing](https://arxiv.org/abs/1510.00726)"
    - "A gentle introduction to LSTMs is available [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
    - "This is one of the papers that have introduced tree LSTM models: Kai Sheng Tai, Richard Socher, and Christopher D. Manning. [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://aclweb.org/anthology/P/P15/P15-1150.pdf)" 
  video: https://webcolleges.uva.nl/Mediasite/Play/f02a9afa14a04197b9af4a2a65f0ad741d
  code: 
  data: 
-  
  layout: lecture
  selected: y
  date: 2020-11-25
  img: LiveQA
  uid: qa3
  title: "Live Q & A: Semantics"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this Q & A session, we will discuss various aspects of lexical, distributional and compositional semantics."
  background:
  discussion:
  slides: resources/slides/NLP1-LiveQA3.pdf
  video: 
  further:   
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: Week 5 (released on 2020-11-25)
  img: Discourse
  uid: lec8
  title: "Discourse processing"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss discourse processing, i.e. modelling larger text fragments."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture8.pdf
  further: 
    - "Chapter 22: [Coreference resolution](https://web.stanford.edu/~jurafsky/slp3/22.pdf) in Jurafsky and Martin (3rd edition)."
    - "Chapter 23: [Discourse coherence](https://web.stanford.edu/~jurafsky/slp3/23.pdf) in Jurafsky and Martin (3rd edition)." 
  video: https://webcolleges.uva.nl/Mediasite/Play/fc8e1d6f4dc14acc9ea7ca7fcca4703d1d
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: Week 5 (released on 2020-11-25)
  img: Summarization 
  uid: lec10
  title: "Language generation and summarisation (part 1: main lecture)"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will talk about language generation and cover a particular language generation task, text summarisation, in more detail."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture9.pdf
  further: 
    - "A survey of recent summarisation techniques is available [here](https://arxiv.org/pdf/1804.04589.pdf)"
    - "An introduction to sequence-to-sequence models: [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)"
  video: https://webcolleges.uva.nl/Mediasite/Play/54d99bd9dcdd476a8b3543ecfc6110cc1d
  code: 
  data:
-
  layout: lecture
  selected: y
  date: Week 5 (released on 2020-11-25)
  img: Summarization 
  uid: lec10
  title: "Language generation and summarisation (part 2: evaluation)"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will talk about language generation and cover a particular language generation task, text summarisation, in more detail."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture9.pdf
  further: 
    - "A survey of recent summarisation techniques is available [here](https://arxiv.org/pdf/1804.04589.pdf)"
    - "An introduction to sequence-to-sequence models: [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)"
  video: https://webcolleges.uva.nl/Mediasite/Play/5e8d240077ed433ab05302f99d21e86f1d?playFrom=5790000&duration=629000
  code: 
  data:  
-  
  layout: lecture
  selected: y
  date: 2020-12-02
  img: LiveQA
  uid: qa4
  title: "Live Q & A: Modelling discourse and summarization"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this Q & A session, we will discuss modelling discourse and summarization."
  background:
  discussion:
  slides: resources/slides/NLP1-LiveQA4.pdf
  video: 
  further:   
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: Week 6 (released on 2020-12-02)
  img: dialogue
  uid: lec9
  title: "Dialogue modelling"
  instructor: "Recorded lecture by Raquel Fernandez"
  note: 
  abstract: >
    "In this guest lecture, we will introduce an important NLP application -- dialogue modelling."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture-dialogue.pdf
  further:
    - "Chapter 26: [Dialogue systems and chatbots](https://web.stanford.edu/~jurafsky/slp3/26.pdf) in Jurafsky and Martin (3rd edition)."
  video: https://webcolleges.uva.nl/Mediasite/Play/e4bf96da1a934346a2c2756d03d672741d
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: Week 6 (released on 2020-12-02)
  img: Twitter
  uid: lec12
  title: "NLP and social media analysis"
  instructor: "Recorded lecture by Reshmi G. Pillai"
  note: 
  abstract: >
    "We will discuss the applications of NLP in the area of social media analysis and the challenges associated with processing the language of social media."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture12.pdf
  further: 
  video: https://webcolleges.uva.nl/Mediasite/Play/ee92c5a252384f91b4b1662bbbc724db1d
  code: 
  data:   
-
  layout: lecture
  selected: y
  date: Week 6 (released on 2020-12-02)
  img: bayes-nlp
  uid: lec13
  title: "Foundations of Bayesian NLP"
  instructor: "Recorded lecture by Wilker Aziz"
  note: 
  abstract: |
      In this lecture, we will discuss the differences between frequentism and Bayesian modelling. We will discuss the concept of a prior and Bayesian inference. The model we will use to illustrate concepts is the Dirichlet-Multinomial model, the base for models such as Bayesian mixture models, HMM, and LDA. For approximate inference, we will discuss MCMC and in particular Gibbs sampling.
  background:
  discussion:
  slides: resources/slides/NLP1-lecture13.pdf
  further: |
      * For a POS tagging model: [A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging](http://aclweb.org/anthology/P07-1094) 
      * For a PCFG model: [Bayesian Inference for PCFGs via Markov chain Monte Carlo](http://aclweb.org/anthology/N07-1018)
      * A very special type of mixture model: [Bayesian Word Alignment for Statistical Machine Translation](http://aclweb.org/anthology/P11-2032)
      * The classict of all times: [Latent Dirichlet Allocation](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
        * This paper focusses on a different type of approximate inference technique (not MCMC, but rather variational inference), in ML4NLP we cover it in great detail (in particular, this is the class of algorithms we use to do probabilistic modelling with neural networks)
      * If you are interested in Bayesian non-parametric methods for NLP, check Sharon Goldwater's [thesis](https://homepages.inf.ed.ac.uk/sgwater/papers/thesis_1spc.pdf), it's remarkably well written and clear!
  video: https://webcolleges.uva.nl/Mediasite/Play/5ee727cc84ac4ea4a6117052410151aa1d
  code: 
  data:
-  
  layout: lecture
  selected: y
  date: 2020-12-09
  img: LiveQA
  uid: qa5
  title: "Live Q & A: Dialogue modelling (1pm) and NLP & Social media (2pm)"
  instructor: "Raquel Fernandez and Reshmi G. Pillai"
  note: 
  abstract: >
    "In this Q & A session, we will discuss dialogue modelling."
  background:
  discussion:
  slides: 
  video: 
  further:   
  code: 
  data:      
-  
  layout: lecture
  selected: y
  date: 2020-12-11
  img: LiveQA
  uid: qa6
  title: "Live Q & A: Bayesian NLP (3pm)"
  instructor: "Wilker Aziz"
  note: 
  abstract: >
    "In this Q & A session, we will discuss Bayesian NLP."
  background:
  discussion:
  slides: 
  video: 
  further:   
  code: 
  data:
-
  layout: lecture
  selected: y
  date: 2020-12-11
  img: Live-1
  uid: lec-final
  title: "Live lecture: Summary of the course (4pm)"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this mini-lecture, we will give you a brief summary of the course and how different areas we have studied interface."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture14.pdf
  further:
  video: 
  code: 
  data:   
-
