-
  layout: lecture
  selected: y
  date: 2020-10-28
  img: Live-1
  uid: intro
  title: "Live lecture: Introduction"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "Introduction to the course"
  background:
  discussion:
  slides: resources/slides/NLP1-lecture1.pdf
  video: 
  further: 
    - "Chapter 4: [Naive Bayes classification and sentiment](https://web.stanford.edu/~jurafsky/slp3/4.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: Week 1 (released on 2020-10-28)
  img: langmodels
  uid: lec2-1
  title: "Language models"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss language models, i.e. modelling word sequences."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture2.pdf
  video: https://webcolleges.uva.nl/Mediasite/Play/ed24f15464dc43dcbafdb9e8c0c700d81d
  further: 
    - "Chapter 3: [Language modelling with n-grams](https://web.stanford.edu/~jurafsky/slp3/3.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:
-
  layout: lecture
  selected: y
  date: Week 1 (released on 2020-10-28)
  img: PoS
  uid: lec2-2
  title: "Part-of-speech tagging"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss part-of-speech tagging, i.e. assigning to each word its grammatical category."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture2.pdf
  video: https://webcolleges.uva.nl/Mediasite/Play/c44f96cb9b18474ab23b66a40691fa641d
  further: 
    - "Chapter 8: [Part-of-speech tagging](https://web.stanford.edu/~jurafsky/slp3/8.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:  
-  
  layout: lecture
  selected: y
  date: 2020-11-04
  img: LiveQA
  uid: qa2
  title: "Live Q & A: Language models and part-of-speech tagging"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this Q & A session, we will discuss language models and part-of-speech tagging."
  background:
  discussion:
  slides: 
  video: 
  further:   
  code: 
  data:     
-
  layout: lecture
  selected: y
  date: Week 2 (released on 2020-11-04)
  img: Morphology
  uid: lec3
  title: "Morphological processing"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss morphological processing"
  background:
  discussion:
  slides: resources/slides/NLP1-lecture3.pdf
  video: https://webcolleges.uva.nl/Mediasite/Play/4bd1dada1d6143369f90057ae8e6358f1d
  further: 
    - "Lecture notes on moprphology are available [here](https://cl-illc.github.io/nlp1/resources/slides/Morphology-notes.pdf)"
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: Week 2 (released on 2020-11-04)
  img: Parsing
  uid: lec4
  title: "Syntax and formal grammars"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, I will introduce syntax and formal grammars."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture4.pdf
  video: https://webcolleges.uva.nl/Mediasite/Play/60b48ad9886f4e0aa997165b58ed43ff1d
  further: 
     - "Chapter 12: [Constituency grammars](https://web.stanford.edu/~jurafsky/slp3/12.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:    
-  
  layout: lecture
  selected: y
  date: Week 2 (released on 2020-11-04)
  img: Parsing
  uid: lec4
  title: "Syntactic parsing"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss syntactic parsing."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture4.pdf
  video: https://webcolleges.uva.nl/Mediasite/Play/5aafeaacb2944e409b7f1134b086f9dd1d
  further: 
     - "Chapter 13: [Constituency parsing](https://web.stanford.edu/~jurafsky/slp3/13.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:    
-  
  layout: lecture
  selected: y
  date: 2020-11-11
  img: LiveQA
  uid: qa2
  title: "Live Q & A: Morphology and syntax"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this Q & A session, we will discuss morphological processing and syntactic parsing."
  background:
  discussion:
  slides: resources/slides/NLP1-liveQA-2.pdf
  video: 
  further:   
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: Week 3 (released on 2020-11-11)
  img: WordNet
  uid: lec4-2
  title: "Lexical semantics"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss lexical semantics, i.e. modelling the meaning of words."
  background:
  discussion:
  slides: 
  video: 
  further: 
     - "Chapter 19: [Word Senses and WordNet](https://web.stanford.edu/~jurafsky/slp3/19.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: Week 3 (released on 2020-11-11)
  img: vectors
  uid: lec5
  title: "Distributional semantics"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will introduce statistical models of word meaning"
  background:
  discussion:
  slides: 
  video: 
  further: 
    - "Chapter 6: [Vector semantics and embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: Week 3 (released on 2020-11-11)
  img: skip-gram
  uid: lec6
  title: "Generalisation and word embeddings"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss generalisation from words to semantic classes and learning dense vector representations - word embeddings."
  background:
  discussion:
  slides: 
  further: 
    - "Chapter 6: [Vector semantics and embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf) in Jurafsky and Martin (3rd edition)."
    - "A gentle introduction to neural networks can be found [here](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)"
    - "The following paper provides a nice explanation of skip-gram with negative sampling: Yoav Goldberg and Omer Levy. [word2vec Explained: Deriving Mikolov et al.â€™s Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722.pdf)"
  video: 
  code: 
  data:   
-
  layout: lecture
  selected: y
  date: 2020-11-18
  img: Live-1
  uid: lec7
  title: "Live lecture: Compositional semantics and sentence representations"
  instructor: "Mario Giulianelli"
  note: 
  abstract: >
    "In this lecture, we will discuss compositional semantics, i.e. modelling the meaning of phrases and sentences, and learning neural representations of sentences."
  background:
  discussion:
  slides: 
  further: 
    - "Chapter 7: [Neural networks and neural language models](https://web.stanford.edu/~jurafsky/slp3/7.pdf) in Jurafsky and Martin (3rd edition)."
    - "Chapter 9: [Sequence processing with recurrent neural networks](https://web.stanford.edu/~jurafsky/slp3/9.pdf) in Jurafsky and Martin (3rd edition)."
    - "A good and general reference for Neural Networks in NLP: Yoav Goldberg. [A Primer on Neural Network Models for Natural Language Processing](https://arxiv.org/abs/1510.00726)"
    - "A gentle introduction to LSTMs is available [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
    - "This is one of the papers that have introduced tree LSTM models: Kai Sheng Tai, Richard Socher, and Christopher D. Manning. [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://aclweb.org/anthology/P/P15/P15-1150.pdf)" 
  video: 
  code: 
  data: 
-  
  layout: lecture
  selected: y
  date: 2020-11-25
  img: LiveQA
  uid: qa2
  title: "Live Q & A: Semantics"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this Q & A session, we will discuss various aspects of lexical, distributional and compositional semantics."
  background:
  discussion:
  slides: 
  video: 
  further:   
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: Week 5 (released on 2020-11-25)
  img: Discourse
  uid: lec8
  title: "Discourse processing"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss discourse processing, i.e. modelling larger text fragments."
  background:
  discussion:
  slides: 
  further: 
    - "Chapter 22: [Coreference resolution](https://web.stanford.edu/~jurafsky/slp3/22.pdf) in Jurafsky and Martin (3rd edition)."
    - "Chapter 23: [Discourse coherence](https://web.stanford.edu/~jurafsky/slp3/23.pdf) in Jurafsky and Martin (3rd edition)." 
  video: 
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: Week 5 (released on 2020-11-25)
  img: Summarization 
  uid: lec10
  title: "Language generation and summarisation"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will talk about language generation and cover a particular language generation task, text summarisation, in more detail."
  background:
  discussion:
  slides: 
  further: 
    - "A survey of recent summarisation techniques is available [here](https://arxiv.org/pdf/1804.04589.pdf)"
    - "An introduction to sequence-to-sequence models: [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)"
  video: 
  code: 
  data:  
-  
  layout: lecture
  selected: y
  date: 2020-12-02
  img: LiveQA
  uid: qa2
  title: "Live Q & A: Modelling discourse and summarization"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this Q & A session, we will discuss modelling discourse and summarization."
  background:
  discussion:
  slides: 
  video: 
  further:   
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: Week 6 (released on 2020-12-02)
  img: dialogue
  uid: lec9
  title: "Dialogue modelling"
  instructor: "Recorded lecture by Raquel Fernandez"
  note: 
  abstract: >
    "In this guest lecture, we will introduce an important NLP application -- dialogue modelling."
  background:
  discussion:
  slides: 
  further:
    - "Chapter 26: [Dialogue systems and chatbots](https://web.stanford.edu/~jurafsky/slp3/26.pdf) in Jurafsky and Martin (3rd edition)."
  video:
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: Week 6 (released on 2020-12-02)
  img: bayes-nlp
  uid: lec13
  title: "Foundations of Bayesian NLP"
  instructor: "Recorded lecture by Wilker Aziz"
  note: 
  abstract: |
      In this lecture, we will discuss the differences between frequentism and Bayesian modelling. We will discuss the concept of a prior and Bayesian inference. The model we will use to illustrate concepts is the Dirichlet-Multinomial model, the base for models such as Bayesian mixture models, HMM, and LDA. For approximate inference, we will discuss MCMC and in particular Gibbs sampling.
  background:
  discussion:
  slides: 
  further: |
      * For a POS tagging model: [A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging](http://aclweb.org/anthology/P07-1094) 
      * For a PCFG model: [Bayesian Inference for PCFGs via Markov chain Monte Carlo](http://aclweb.org/anthology/N07-1018)
      * A very special type of mixture model: [Bayesian Word Alignment for Statistical Machine Translation](http://aclweb.org/anthology/P11-2032)
      * The classict of all times: [Latent Dirichlet Allocation](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
        * This paper focusses on a different type of approximate inference technique (not MCMC, but rather variational inference), in ML4NLP we cover it in great detail (in particular, this is the class of algorithms we use to do probabilistic modelling with neural networks)
      * If you are interested in Bayesian non-parametric methods for NLP, check Sharon Goldwater's [thesis](https://homepages.inf.ed.ac.uk/sgwater/papers/thesis_1spc.pdf), it's remarkably well written and clear!
  video: 
  code: 
  data:
-  
  layout: lecture
  selected: y
  date: 2020-12-09
  img: LiveQA
  uid: qa2
  title: "Live Q & A: Dialogue modelling"
  instructor: "Raquel Fernandez"
  note: 
  abstract: >
    "In this Q & A session, we will discuss dialogue modelling."
  background:
  discussion:
  slides: 
  video: 
  further:   
  code: 
  data:      
-  
  layout: lecture
  selected: y
  date: 2020-12-11
  img: LiveQA
  uid: qa2
  title: "Live Q & A: Bayesian NLP"
  instructor: "Wilker Aziz"
  note: 
  abstract: >
    "In this Q & A session, we will discuss Bayesian NLP."
  background:
  discussion:
  slides: 
  video: 
  further:   
  code: 
  data:
